[Pytorch基础知识-CSDN](https://blog.csdn.net/qq_51713698/article/details/126673876)[Python | 菜鸟教程](https://www.runoob.com/python/python-intro.html)[Pytorch中文文档](https://pytorch-cn.readthedocs.io/zh/latest/) [如何写出更具有Python风格的代码](https://blog.51cto.com/u_15273875/2917312)
> [!note] 注释
> 相关概念：
> 监督学习：
> 非监督学习：

##### 1 线性回归

**1.1 计算公式：**
- 一元线性回归：假设$y$与$x$具有线性关系，一元线性回归模型可以表示为：
$$y = \beta_0 + \beta_1x + \epsilon$$
> [!note] 注释
> 其中，$\beta_0$是截距，$\beta_1$是斜率，$\epsilon$是误差.

   变量与单个变量的线性回归示意图如下：

![[assets/Linear Rgression.png|300]]

- 多元线性回归：当然，以上公式只能计算某个变量与单个变量的线性回归关系，拓展到高维空间，假设$y$与$x_1, x_2, ..., x_p$具有线性关系，则多元线性回归的计算公式为：
$$y = \beta_0 + \beta_1x_1 + \beta_2x_2 + ... + \beta_px_p + \epsilon$$

- 主成分回归：主成分回归实际上就是将主成分作为回归的参数，主成分回归通过数据降维，将降维后的数据代入线性回归方程，主要解决共线性的问题，但是结果缺乏可解释性：
$$y = \beta_0 + \beta_1k_1 + \beta_2k_2 + ... + \beta_pk_p + \epsilon$$
> [!note] 注释
> 其中的$k$为主成分，其实就是把多元线性回归的$x$换成了主成分，可根据实际需要选择前$p$个主成分，参见主成分分析部分

**1.2 参数估计：**

- 线性回归采用最小二乘法进行参数估计，以一元线性回归为例，设$n$组数据点为$(x_1, y_1), (x_2, y_2), \ldots, (x_n, y_n)$，则$\beta_1$和$\beta_0$的计算公式为：
$$\beta_1 = \frac{\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n}(x_i - \bar{x})^2}$$
$$\beta_0 = \bar{y} - \beta_1\bar{x}$$
> [!note] 注释
> 其中，$\bar{x}$和$\bar{y}$分别为$x_i$和$y_i$的平均值。

~~多元线性回归的最小二乘法略~~

##### 2 逻辑回归
逻辑回归(Logistic Regression)一般用于解决二分类问题，其实它就是一个线性回归加上一个$sigmoid$函数

**计算公式：**

- 假设$y$与$x_1, x_2, ..., x_p$之间存在线性关系，则线性回归计算公式为：
$$y = \beta_0 + \beta_1x_1 + \beta_2x_2 + ... + \beta_px_p + \epsilon$$
- 而$sigmoid$函数的计算公式为：
$$
z = \frac{1}{1+e^{-y}}
$$
- 将$y$代入得到逻辑回归的公式：
$$
z = \frac{1}{1 + e^{-(\beta_0 + \beta_1x_1 + ... + \beta_px_p)}}
$$

**参数估计：**

- 逻辑回归的损失函数：
$$
C = -[y ln(a)+(1-y)ln(1-a)]
$$
- 这个方程似乎与逻辑斯蒂方程有联系，逻辑斯蒂方程的微分形式：
$$
\frac{dN}{dt} = r\frac{N-K}{K}
$$

##### 3 决策树

$CART$(Classification And Regression Trees)算法是最常见的决策树算法，按照任务类型可分为回归树和分类树。

###### 3.1 回归树
- 回归任务使用误差平方和(SSE)作为损失函数。给定训练数据集$D={(x_1,y_1),(x_2,y_2),…,(x_n,y_n)}$，其中$x_i$是特征向量，$y_i$是目标值，$CART$算法试图找到一个决策树$T$，使得损失函数$L(T,D)$最小，即：
$$L(T,D)=\sum_{i=1}^{n}(y_i-\hat{y}_i)^2$$
> [!note] 注释
> 其中，$\hat{y}_i$是决策树预测的值。在每个节点上，CART算法都会尝试找到最佳的分割点，将数据分为两部分，然后递归地在子节点上重复这个过程，直到满足停止条件。

###### 3.2 分类树 
- 分类树使用$Gini$指数度量节点纯度。给定一个节点$N$，其$Gini$指数为：
$$Gini(N)=1-\sum_{k=1}^{K}p_k^2$$
> [!note] 注释
> 其中，$p_k$是属于第$k$类的样本比例。在每个节点上，算法的目标是使得分割后的子节点的Gini指数之和最小。
> 为了防止过拟合，CART算法通常会使用预剪枝或后剪枝技术。预剪枝是在构建决策树的过程中，当节点的样本数小于某个阈值或者节点的$Gini$指数小于某个阈值时，就停止分裂该节点。后剪枝是在构建完决策树后，从叶子节点开始，如果将该节点及其子节点剪掉，对训练数据集的预测误差没有明显增加，则将该节点及其子节点剪掉。

##### 4 随机森林

- 随机森林(Random Forest)是一种集成学习方法，基础单元是决策树。单个决策树容易过拟合，而随机森林通过构建多个决策树并采用投票机制或平均预测值来提高泛化能力。

- 自助采样是从$n$个样本中有放回地抽取$n$个样本，得到一个新的训练集。这个新的训练集可能包含重复的样本，也可能遗漏部分原始样本。

- 在每个决策树的节点分裂时，不是考虑所有特征，而是从所有特征中随机选择一部分特征，然后在这部分特征上寻找最佳分割点。

- 如果原始数据集大小为$n$，那么某个样本被选中的概率为$1 - (1 - \frac{1}{n})^n$，随着$n$增大，这个概率趋近于$1 - e^{-1} \approx 0.632$。

- 分类任务：设$k$为随机森林中决策树的数量，$T_i(x)$为第$i$棵树对输入$x$的预测，那么随机森林的预测为：
$$F(x) = \arg\max_{c \in C} \sum_{i=1}^{k} I(T_i(x) = c)$$

- 其中，$C$是所有可能的类别集合，$I(\cdot)$是指示函数。

- 回归任务：设$k$为随机森林中决策树的数量，$T_i(x)$为第$i$棵树对输入$x$的预测，那么随机森林的预测为：
$$F(x) = \frac{1}{k}\sum_{i=1}^{k} T_i(x)$$

##### 5 支持向量机

支持向量机(Support Vector Machine, SVM)的目标是找到一个超平面，使得两个类别之间的间隔最大化

![[assets/SVM.png|300]]

**线性可分情况**

（1）任意一条线可用下面的线性方程来表示：
$$ w^T x + b = 0 $$
> [!note] 注释
> 其中，$w$是与超平面垂直的权重向量，$b$是偏置项。为了最大化分类间隔，我们需要找到满足以下条件的$w$和$b$：

（2）在平面上，点$(𝑥, 𝑦)$到直线$𝐴𝑥 + 𝐵𝑦 + 𝐶 =0$的距离是：

$$\frac{|A x+B y+C|}{\sqrt{A^{2}+B^{2}}}$$

推广到$n$维空间，点到超平面的距离为：

假设我们有一组数据点$(x_i, y_i)$，对于所有训练样本$(x_i, y_i)$，有：

$$ y_i(w^T x_i + b) \geq 1 $$

同时，我们希望最小化$w$的范数$\|w\|$，即：

$$ \min_{w,b} \frac{1}{2}\|w\|^2 $$

这构成了一个优化问题：

$$ \min_{w,b} \frac{1}{2}\|w\|^2 $$
$$ \text{s.t. } y_i(w^T x_i + b) \geq 1, \forall i $$

使用拉格朗日乘子法，我们可以将约束优化问题转换为无约束优化问题：

$$ L(w,b,\alpha) = \frac{1}{2}\|w\|^2 - \sum_{i=1}^{m}\alpha_i[y_i(w^T x_i + b) - 1] $$
> [!note] 注释
> 其中，$\alpha_i \geq 0$是拉格朗日乘子。通过求解这个优化问题，得到$w$和$b$的最优值

**非线性可分情况**

在实际应用中，数据往往不是线性可分的。此时，可以通过引入核函数（Kernel Function）将数据映射到高维空间，使之变得线性可分。

常见的核函数有：

(1) 线性核：$$K(x, x') = x^T x'$$

(2) 多项式核：$$K(x, x') = (x^T x' + c)^d$$

(3) 高斯径向基函数核：$$K(x, x') = \exp(-\gamma\|x-x'\|^2)$$

SVM通过寻找最大间隔超平面来实现分类，对于非线性可分的数据，通过核技巧将其映射到高维空间，从而解决复杂分类问题

##### 6 马尔可夫链

马尔可夫链(Markov Chain)是一种统计模型，用于描述一系列状态序列，在序列中，每个状态只依赖于前一个状态。这种依赖关系通过转移概率矩阵来表示，该矩阵定义了从一个状态转移到另一个状态的概率。计算示例如下：

(1) 定义状态空间和转移概率矩阵：状态空间$S$包含了马尔可夫链中所有可能的状态，可以表示为 $S={s_1, s_2, \ldots, s_n}$。考虑一个简单的天气预报问题，状态空间为$S={\text{晴}, \text{雨}}$，转移概率矩阵$P$如下：

$$
P =
\begin{pmatrix}
0.8 & 0.2 \\
0.4 & 0.6
\end{pmatrix}
$$

其中，$P_{11} = 0.8$表示晴天后一天还是晴天的概率，$P_{12} = 0.2$表示晴天后一天转为雨天的概率，依此类推。

(2) 初始分布：设初始分布为$\pi_0 = (0.7, 0.3)$，即初始时晴天的概率为$0.7$，雨天的概率为$0.3$。

(3) 状态转移计算：转移概率矩阵 $P$ 是一个$n \times n$的矩阵，其中$P_{ij}$ 表示从状态$s_i$转移到状态$s_j$的概率。矩阵的每一行元素之和为$1$。第一天结束时的状态分布为：
$$
\pi_1 = \pi_0P = (0.7, 0.3)
\begin{pmatrix}
0.8 & 0.2 \\
0.4 & 0.6
\end{pmatrix}
= (0.7 \times 0.8 + 0.3 \times 0.4, 0.7 \times 0.2 + 0.3 \times 0.6) = (0.68, 0.32)
$$

(4) 稳态分布计算：稳态分布$(\pi^*)$是指在足够长时间之后，马尔可夫链状态分布趋于一个不变的分布，满足 $\pi^* = \pi^*P$ 和$\sum_{i=1}^{n}\pi^*_i=1$。为了找到稳态分布，我们需要解方程 $\pi^* = \pi^*P和\sum_{i=1}^{n} \pi^*_i = 1。设\pi^* = (\pi^*_1, \pi^*_2)$，则有：

$$
\begin{cases}
\pi^*_1 \times 0.8 + \pi^*_2 \times 0.4 = \pi^*_1 \\
\pi^*_1 \times 0.2 + \pi^*_2 \times 0.6 = \pi^*_2 \\
\pi^*_1 + \pi^*_2 = 1
\end{cases}
$$

通过解这个方程组，我们得到稳态分布$\pi^*$。具体解法：先从从第一个方程中，我们可以得出$\pi^*_2 = 2\pi^*_1 - \pi^*_1 = \pi^*_1$。然后将 $\pi^*_2 = \pi^*_1$ 代入第三个方程，得到 $2\pi^*_1=1，从而\pi^*_1 = 0.5$。因此，$\pi^*_2 = 0.5$。综上，稳态分布为 $\pi^*=(0.5, 0.5)$。
##### 7 朴素贝叶斯
略 看概率论
##### 8 K近邻算法
##### 9 K均值聚类

- **计算步骤：**

   (1) 设定参数：
   
   设定簇的数量K并随机选择K个数据点作为初始聚类中心
 
  (2) 分配数据：
  
  分配数据点到最近的聚类中心：  对于每个数据点，计算其与每个聚类中心的距离，将其分配给距离最近的聚类中心所在的簇。对每个点$x_i$，找到最近的质心$c_j$，依据公式
 $$d(x_i, c_j) = \sqrt{\sum_{d=1}^{D}(x_{id} - c_{jd})^2}$$
 
   (3) 更新聚类：
   
   对于每个簇，计算该簇中所有数据点的平均值，将这个平均值作为新的聚类中心。对每个聚类$C_j$，计算新质心
$$c_j = \frac{1}{|C_j|} \sum_{x_i \in C_j} x_i$$
 (4) 重复步骤：
 
   重复这个过程，直到聚类中心不再发生显著变化，或者达到预定的迭代次数。

##### 10 梯度提升机

##### 11 主成分分析
 
- 主成分分析(Principal Component Analysis, PCA)是数据分析中最常用的数据降维方法。其基本思想是将多个可能存在相关性的变量处理成多个互不相关的变量，这些处理后的变量称为主成分，同时根据需要从处理后的变量中去掉部分不重要的信息，从而达到数据降维的目的。具体计算步骤如下：

   (1) 首先，构建样本数据矩阵：
$$
X = \begin{pmatrix}
	x_{11} & x_{12} &\cdots& x_{1m} \\
	x_{21} & x_{22} &\cdots& x_{2m} \\
	\vdots &\vdots&\ddots&\vdots\\
	x_{n1} & x_{n2} &\cdots& x_{nm} \\
\end{pmatrix}
$$
> [!note] 注释
> 其中，每一行为一个样本，$n$为样本数，$m$为样本特征数

   (2) 使用$Z$-$score$对矩阵$X$进行标准化的到标准化后的矩阵$B$：
$$z_{ij} = \frac{x_{ij}-\overline x_j}{S_j}$$
$$
Z = \begin{pmatrix}
	z_{11} & z_{12} &\cdots& z_{1m} \\
	z_{21} & z_{22} &\cdots& z_{2m} \\
	\vdots &\vdots&\ddots&\vdots\\
	z_{n1} & z_{n2} &\cdots& z_{nm} \\
\end{pmatrix}
$$
> [!note] 注释
> 其中，$\overline x_j$为样本第$j$列特征的平均值，$S_j$为样本第$j$列特征的标准差

   (3) 求出矩阵$B$的协方差矩阵$C$
$$cov(X,Y)=\frac{1}{n-1}\sum_{i=1}^n X_iY_i$$
$$C=\frac{1}{n-1}Z^TZ$$

   (4) 求出协方差矩阵的特征值及对应的特征向量
$$Cα=λα$$
> [!note] 注释
> 其中$λ$为特征值

   (5) 最后，将特征向量按对应特征值大小从上到下按行排列成矩阵，取前$k$行组成矩阵$K$，$Y = XK$即为降维到$k$维后的数据。
   > [!note] 注释
> 将特征值按从大到小排序，对应的特征向量就是主成分。通常选择前$k$个最大的特征值对应的特征向量作为新的坐标轴，这$k$个坐标轴构成了低维空间。总结：先构建样本数据矩阵，对样本数据标准化，然后求出对应的协方差矩阵，最后计算主成分，提取部分主成分后降维

> [!note] 注释
   > 参考文献：
[支持向量机](https://zhuanlan.zhihu.com/p/382296265)
https://b23.tv/vlPHW3K
https://zhuanlan.zhihu.com/p/563055333
[核函数](https://blog.csdn.net/l8947943/article/details/127520662)