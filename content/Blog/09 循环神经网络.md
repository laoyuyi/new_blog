#### 循环神经网络

循环神经网络[](https://www.bilibili.com/video/BV1z5411f7Bm/?spm_id_from=333.337.search-card.all.click)（Recurrent Neural Network, RNN）又叫递归神经网络，很简单，相比于普通的全连接神经网络就是把上一次训练的权重与这一次训练的输出值同时输入神经网络。循环神经网络的结构如下：

![[assets/RNN.png|400]]

- **计算公式：**
$$Y_n=softmax(\sum^{n}_{n=1}tanh(UX_n​+WS_{n−1}​))$$
  > [!note] 注释
> $X_n$为输入内容，$Y_n$为输出内容，$U、V、W$为权重，$S_n​$为神经元，$c$为输出的个数​.
> 根据问题的需要，循环神经网络的隐层可以是全连接的，也可以是一对一的。此外，假设每一层级之间的权重是相同的，可降低计算量。循环神经网络的常见变体有长短时记忆神经网络和门控循环单元。

##### 长短时记忆神经网络

长短时记忆神经网络(long short-term memory neural network, LSTM)是一种主要用于处理时序问题的神经网络结构，由循环神经网络改进而来。相比于传统的循环神经网络，长短时记忆神经网络能够深入挖掘时间序列数据中的固有规律并具有长时记忆功能。$LSTM$的结构包括输入门$i_t$，遗忘门$f_t$，输出门$o_t$，其结构图如下：

![[assets/LSTM.png|400]] 

- **遗忘门：**
$$f_t=σ(W_{xf}x_t+b_{xf}+W_{hf}h_{t-1}+b_{hf})$$
  > [!note] 注释
> 式中，输入值$x_t$和上一时刻的输出值$h_{t-1}$经过$Sigmoid$激活函数后，生成一个$0$~$1$之间的向量，遗忘门通过对记忆细胞的点乘操作对记忆内容进行选择性遗忘，若向量中某元素的值为$0$，则经过点乘后记忆细胞中对应值全部遗忘，若向量中某元素的值为$1$，则全部保留。$W_{xf}$和$W_{hf}$分别为遗忘门的权重，$b_{xf}$和$b_{hf}$为偏置项。

- **记忆门：**
$$i_t=σ(W_{xi}x_t+b_{xi}+W_{hi}h_{t-1}+b_{hi})$$
$$g_t=tanh(W_{xg}x_t+b_{xg}+W_{hg}h_{t-1}+b_{hg})$$
  > [!note] 注释
> 式中，输入值$x_t$和上一时刻的输出值$h_{t-1}$经过$Sigmoid$激活函数后，生成一个0~1之间的向量，输入值$x_t$和上一时刻的输出值$h_{t-1}$再经过$tanh$激活函数后放缩到$[-1, 1]$成为候选细胞，候选细胞只是起到一个过渡的作用，两者之间经过元素相乘运算清除部分信息，这一步相当于选择要记忆什么内容，再经过与记忆细胞的元素相加运算将候选细胞的记忆内容记忆加到记忆细胞中。$W_{xi}$和$W_{hi}$分别为记忆门的权重，$b_{xi}$和$b_{hi}$为偏置项。$W_{xg}$和$W_{hg}$为候选细胞权重，$b_{xg}$和$b_{hg}$为偏置项。

- **记忆细胞：**
$$c_t=f_t⊙c_{t-1}+i_t⊙g_t$$
  > [!note] 注释
> 记忆细胞代表长期记忆，经过与遗忘门的点乘操作选择性遗忘和与记忆门的元素相加操作选择性记忆得到。

- **输出门：**
$$o_t=σ(W_{xo}x_t+b_{xo}+W_{ho}h_{t-1}+b_{ho})$$
$$h_t=o_t⊙tanh(c_t)$$
  > [!note] 注释
> 输出门选择要输出什么内容，输入值$x_t$和上一时刻的输出值$h_{t-1}$经过sigmoid激活函数后生成元素为$0$~$1$之间的向量。$W_{xo}$和$W_{ho}$为权重，$b_{xo}$和$b_{ho}$为偏置项。输入门再与记忆细胞进行点乘操作，将记忆细胞中的信息提取出来，得到最终输出$h_t$，$h_t$再参与到下一时刻的训练。

- **具体流程：**
![[assets/LSTM_Gif.gif|400]]
  > [!note] 注释
> LSTM的门展开后就是Dense层
> LSTM应该记忆什么遗忘什么取决于门的权重，由网络训练得到

##### 门控循环单元

门控循环单元(gated recurrent unit, GRU)由长短时记忆神经网络改进而来，门控循环单元与长短时记忆神经网络效果相似，但门控循环单元模型更简洁，$GRU$的结构与计算公式如下所示：

![[assets/GRU.png|400]]

- **重置与更新门：**
$$ R_t​=σ(X_t​W_{xr}​+H_{t−1}​W_{hr}​+b_r​)$$ 
$$ Z_t=σ(X_tW_{xz}+H_{t-1}W_{hz}+b_z)$$ 
  > [!note] 注释
> $R_t, Z_t$分别为时刻$t$的重置门和更新门，$H_t$是时刻$t$的输出，$X_t$是时刻$t$的输入，$W_{xr}和W_{hr}$为重置门的权重，$b_r和b_r$为重置门的偏置项，$W_{xz}和W_{hz}$为更新门的权重，$b_z和b_z$为更新门的偏置项，$R_t, Z_t$均为0~1之间的向量。

- **隐藏状态**
$$ \tilde H_t=tanh(X_t​W_{xh​}+(R_t​⊙H_{t−1}​)W_{hh​}+b_h$$ 
$$ H_t=Z_t​⊙H_{t−1}​+(1−Z_t​)⊙\tilde H_t$$ 
  > [!note] 注释
> $\tilde H_t$为候选隐藏状态，$Z_t$表示遗忘，反之$(1−Z_t​)$表示记忆，$W_{xh​}$和$W_{hh​}$为隐藏状态的权重，$b_h$为隐藏状态的偏置项。
> 有权重的地方就表明这里有一个$Dense$层。如果将重置门$R_t$设为$1$，更新门设为$Z_t$设为$0$，就是一个标准的循环神经网络。

- **具体流程：**
![[assets/GRU_Gif.gif|400]]

  > [!note]- 注释
> 参考资料：
> https://blog.csdn.net/zyf918/article/details/136172798