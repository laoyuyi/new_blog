#### 深度神经网络

- 神经网络(Artificial Neural Network, ANN)是一种通过模拟生物神经网络进行数据拟合的机器学习算法模型，超过三层的神经网络称为深度神经网络或深度学习。它由大量的人工神经元组成，这些神经元相互连接并通过权重来传递信息。

##### 数据归一化

- 数据进入神经网络前需要进行归一化步骤。常用的归一化方法有$LayerNorm$、$BatchNorm$、$RMSNorm$等。
> [!note] 注释
> 注意：归一化与标准化是两个不同的概念。归一化是指将数据缩放到$[0, 1]$或$[-1, 1]$区间内，标准化指的是将数据转换成均值为$0$，标准差为$1$的分布。

  (1) $LayerNorm$：设$x_{ij}$表示第$i$个样本的第$j$个特征，$n$为特征数量，则$LayerNorm$的计算公式为：
$$
y_{ij} = \gamma \frac{x_{ij} - \frac{1}{n} \sum_{j=1}^{n} x_{ij}}{\sqrt{\frac{1}{n} \sum_{j=1}^{n} (x_{ij} - \frac{1}{n} \sum_{j=1}^{n} x_{ij})^2 + \epsilon}} + \beta
$$
> [!note] 注释
> 其中，$\epsilon$是一个小常数，用来防止除以零的情况。可学习参数$\gamma$和$\beta$进行用于缩放和平移，使得模型可以学习到最佳的归一化效果。
> 每个元素的值减去样本每个特征的平均值，再除以标准差，最后再进行缩放和平移。
  
  (2) $BatchNorm$：设每个特征的均值为$μ$，方差为$σ^2$，则$BatchNorm$的计算公式为
$$
y_{ij} = \gamma \frac{x_i - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta
$$
> [!note] 注释
> 这里的$\gamma$和$\beta$是可学习的参数，对于每个特征维度都有一个对应的$\gamma$和$\beta$值。在训练过程中，这些参数会通过反向传播算法进行更新。

  (3) $RMSNorm$：
$$
y_{ij} = \frac{x}{\sqrt{Mean(x^2)+ε}}·γ
$$
> [!note] 注释
> $RMSNorm$比$Layernorm$计算更简便，节约了计算速度，

##### 前向传播过程

- 前向传播(Forward Propagation)指神经网络中从输入层到输出层的数据传递过程。在这个过程中，输入数据从输入神经元开始，经过多层神经元的加权求和后经过激活函数生成输出结果。具体步骤如下：
![[assets/DNN.png|400]]
$$y(x)=g(\sum_{i=1}^{n}w_ix_i+b)$$
> [!note] 注释
> 其中，$y(x)$表示神经网络的输出，$g$表示‌激活函数，$w_i$表示第$i$个输入数据的权重，$x_i$表示第$i$个输数据号，$b$为偏置项。前向传播的主要目的是基于当前的权重和偏置参数，计算模型对于给定输入的预测输出。

- **激活函数**

  在神经网络中除了线性层外往往还需要加入激活函数，没有激活函数的神经网络只是线性函数，无法拟合曲线，任何一条曲线都能用多条线性函数来逼近，而激活函数使神经网络具有分段的能力，根据万能逼近原理(universal approximation theorrm)，3层神经网络可以逼近任意曲线[](https://zhuanlan.zhihu.com/p/385531651)。以下是神经网络中最常见的三个激活函数：
  (1) $Sigmoid$、$tanh$、$Relu$激活函数：
$$
\begin{align}
	&Sigmoid(x)=\frac{1}{1+e^{-x}}\\
	&Tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}\\
	&Relu(x)=
	\begin{cases}
		x,x≥0\\
		0,x<0
	\end{cases}
\end{align}
$$
> [!note] 注释
> $Relu$函数反向传播求导简单，减少计算量

  (2) $GLU$激活函数：
 $$GLU(x)=Sigmoid(W_1x+b_1)⊗(W_2x+b_2) $$
 > [!note] 注释
> $x$是输入向量，$⊗$表示逐元素相乘。$GLU$函数将输入向量通过两个不同的线性变换，得到了两个不同的向量，其中一个向量不经过激活函数，另一个向量经过$Sigmoid$函数后其元素被映射到$0-1$之间，若元素为$1$，则另一向量对应位置的元素不变，若为零则不允许通过，感觉有点类似于$LSTM$的门控结构，个人理解$GLU$的作用类似于$LSTM$中的遗忘门。

  (3) $Swish$激活函数：
$$Swish(x)=x⊗Sigmoid(βx)$$
> [!note] 注释
> $𝛽$是一个可学习的超参数，默认为$1$。$Swish$函数的独特之处在于其非线性和自适应性，当输入接近$0$时，行为类似$ReLU$函数，而在其他区域，其斜率可以根据输入值进行自我调整。相比传统的$ReLU$无法调整的一条直线更好。

  (4) $SwiGLU$激活函数：
$$SwiGLU(x)=Swish(W_1x+b_1)⊗(W_2x+b_2) $$
> [!note] 注释
> $SwiGLU$函数相当于把$GLU$和$Swish$结合在了一起，把$GLU$当中的$Sigmoid$换成了$Swish$，结合了二者的优点，$1+1>2$。
> $SiLU$函数：LlaMa2模型采用的是$SiLU$函数，$SiLU$就是$β=1$时的$Swish$与$GLU$结合的$SwiGLU$函数

- **损失函数**

  神经网络通过计算预测值与真实值之间的误差，再将误差进行反向传播，从而更新权重。
  
  (1) 常见损失函数：常用的损失函数有$MSE$、$RMSE$、$MAE$等，计算公式如下：
$$
\begin{align}
	&\text{MSE}=\frac{1}{n}\sum_{i=1}^{n}(y_i-\hat{y}_i)^2\\
	&\text{RMSE}=\sqrt{MSE}\\
	&\text{MAE}=\frac{1}{n}\sum_{i=1}^{n}|y_i-\hat{y}_i|
\end{align}	
$$
> [!note] 注释
> 其中，$y_i$是第$i$个真实值，$\hat{y}_i$是模型第$i$个预测值，$n$为数据总数。 
> - $MSE$是预测值与真实值之差的平方的平均值，函数通过差值的平方放大误差的影响；
> - $RMSE$实际上是$MSE$的平方根，$RMSE$保留了$MSE$的特性，但它将误差单位转换回原始数据的尺度，使得解释更加直观； 
> - $MAE$是预测值与真实值之差的绝对值的平均值。它比$MSE$更能抵抗异常值的影响，因为绝对值不会像平方那样放大误差。 

  (2) 交叉熵损失 (Cross-Entropy Loss)：

  不同的问题交叉熵损失的计算公式也不同，对于二分类问题，交叉熵损失的计算公式如下： 
$$\text{CE}=-\frac{1}{n}\sum_{i=1}^{n}[y_i\log(p_i) + (1-y_i)\log(1-p_i)]$$
> [!note] 注释
> 其中，$p_i$是模型预测第$i$个观测属于正类的概率，$y_i$是第$i$个观测的真实标签（通常为$0$或$1$）。 

  对于多分类问题，交叉熵损失的计算公式如下： 
$$\text{CE} = -\frac{1}{n}\sum_{i=1}^{n}\sum_{c=1}^{C}y_{ic}\log(p_{ic})$$
> [!note] 注释
> 其中，$C$是类别的数量，$y_{ic}$是指示变量，若第$i$个观测属于第$c$类，则为$1$，否则为$0$，$p_{ic}$是模型预测第$i$个观测属于第$c$类的概率。 
交叉熵损失用于分类问题，特别是当输出是概率分布时。它度量了两个概率分布之间的差异，鼓励模型预测与真实标签尽可能接近的概率分布。

##### 反向传播过程
  
- 神经网络的权重更新通过反向传播实现[](https://zhuanlan.zhihu.com/p/289184911)，参见周志华-机器学习p101