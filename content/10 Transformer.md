#### Transformer

[Transformerä»é›¶è¯¦ç»†è§£è¯»(å¯èƒ½æ˜¯ä½ è§è¿‡æœ€é€šä¿—æ˜“æ‡‚çš„è®²è§£)](https://www.bilibili.com/video/BV1Di4y1c7Zm/?share_source=copy_web&vd_source=59e03da0555efae2883b4a6f21e8f47b)
[æ‰‹æ¨transformer_å“”å“©å“”å“©](https://www.bilibili.com/video/BV1UL411g7aX/?spm_id_from=333.337.search-card.all.click&vd_source=3749948871311220e961685a5ac2a7a7)

---

- Transformerç»“æ„æ¡†æ¶å›¾ï¼š
![[assets/Transformer.png|250]]

- **è¾“å…¥éƒ¨åˆ†** 

  (1) è¯åµŒå…¥($embedding$)ï¼š 
  > [!note] æ³¨é‡Š
  > ä¸¤è¾¹çš„$Inputs$å’Œ$Outputs$åˆ†åˆ«ä»£è¡¨æ ·æœ¬å’Œæ ‡ç­¾æ•°æ®ï¼Œ$Embedding$æ˜¯ä¸€ä¸ªå•å±‚çš„å…¨è¿æ¥å±‚ï¼Œ$Inputs$å’Œ$Outputs$ç»è¿‡$Embedding$çš„è½¬æ¢åå¯ä»¥è¡¨ç¤ºå•è¯æˆ–è€…è¯­å¥çš„è¯­ä¹‰

  (2) ä½ç½®ç¼–ç ($positional$ $encoding$)ï¼š
 
   è¯å‘é‡å¶æ•°ä½ç½®ä½¿ç”¨$\sin$ï¼Œå¥‡æ•°ä½ç½®ä½¿ç”¨$\cos$ï¼Œå°†ä½ç½®ä¿¡æ¯åµŒå…¥è¯å‘é‡ä¸­ï¼ŒåŸå§‹ç‰ˆæœ¬çš„Transformeré‡‡ç”¨ä¸‰è§’å‡½æ•°ç¼–ç ($Sinusoidal$ $Encoding$)ï¼š
$$PE_{(pos,2i)}=\sin(\frac{pos}{10000^{2i/d_{model}}})$$
$$PE_{(pos,2i+1)}=\cos(\frac{pos}{10000^{2i/d_{model}}})$$
$$
\begin{matrix}
	sin & cos & sin & cos & sin & cos\\
	â†“ & â†“ & â†“ & â†“ & â†“ & â†“ &\\
	[0, & 1, & 2, & 3, & 4, & 5, & \cdots &]
\end{matrix}
$$
> [!note] æ³¨é‡Š
> $pos$è¡¨ç¤ºä½ç½®ç´¢å¼•ï¼ˆä»$0$å¼€å§‹ï¼‰ï¼Œ$ğ‘–$æ˜¯è¯åµŒå…¥å‘é‡çš„ç»´åº¦ç´¢å¼•ï¼Œ$ğ‘‘_{ğ‘šğ‘œğ‘‘ğ‘’ğ‘™}$æ˜¯æ¨¡å‹ä¸­è¯åµŒå…¥å‘é‡çš„ç»´åº¦ã€‚$å¸¦æœ‰ä½ç½®ä¿¡æ¯çš„è¯embedding = åŸå§‹çš„è¯embedding+è¯embeddingå¯¹åº”çš„PEè®¡ç®—ç»“æœ$ã€‚RNNå’ŒLSTMæ˜¯å°†æ–‡æœ¬ä¸€ä¸ªè¯ä¸€ä¸ªè¯è¾“å…¥ç½‘ç»œï¼Œè€Œtransformeræ˜¯å¹¶è¡Œè®¡ç®—çš„ï¼Œæ„å‘³ç€è¯ä¸è¯ä¹‹é—´ä¸å­˜åœ¨é¡ºåºå…³ç³»ï¼Œæ‰€ä»¥éœ€è¦å°†ä½ç½®ä¿¡æ¯åµŒå…¥è¯å‘é‡ä¸­ï¼ŒPEåµŒå…¥çš„æ˜¯å´å¯¹ä½ç½®ä¿¡æ¯ï¼Œé€šè¿‡é¢‘ç‡ã€æ³¢é•¿ä¸ä½ç½®çš„å…³ç³»æŠŠä¿¡æ¯åµŒå…¥è¯å‘é‡ä¸­ã€‚


- **Encoderéƒ¨åˆ†**

  æ•´ä¸ªTransformerå¯ä»¥åˆ†ä¸º$Encoder$å’Œ$Decoder$ä¸¤ä¸ªéƒ¨åˆ†ã€‚å…¶ä¸­ï¼Œ$Encoder$å¯åˆ†æˆè¾“å…¥éƒ¨åˆ†ã€æ³¨æ„åŠ›æœºåˆ¶å’Œå‰é¦ˆç¥ç»ç½‘ç»œä¸‰ä¸ªéƒ¨åˆ†

  (1) è‡ªæ³¨æ„åŠ›æœºåˆ¶($Self$-$Attention$)ï¼š[zhihu](https://zhuanlan.zhihu.com/p/410776234)

   $Q(query)ã€K(key)ã€V(value)ä»¥åŠAttention$çš„è®¡ç®—ï¼š
$$Q = X \times W^Q$$
$$K = X \times W^K$$
$$V = X \times W^V$$
$$Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V$$
> [!note] æ³¨é‡Š
> $X$æ˜¯ç»è¿‡è¯åµŒå…¥ä¸ä½ç½®ç¼–ç åçš„è¾“å…¥ï¼Œ$W^Q$ã€$W^K$ã€$W^V$æ˜¯ä¸‰ä¸ªå¯è¢«è®­ç»ƒçš„å‚æ•°çŸ©é˜µã€‚$d_k$æ˜¯é”®å¯¹åº”çš„å‘é‡ç»´åº¦ï¼Œå³$embedding$çš„ç»´åº¦ã€‚å…³äº$Qã€Kã€V$çš„ä½œç”¨ï¼š[â€»](https://www.zhihu.com/question/325839123)ã€‚$QK^T$è¡¨ç¤ºæŸ¥è¯¢æƒé‡ï¼Œ$Q$ä¹˜$K^T$ç›¸å½“äº$Q$ä¸$K$åšæ•°é‡ç§¯æ“ä½œï¼Œæ•°é‡çº§å¯ä»¥åæ˜ ä¸¤ä¸ªå‘é‡çš„ç›¸ä¼¼åº¦ï¼Œç»“æœè¶Šå¤§è¡¨æ˜ä¸¤ä¸ªå‘é‡çš„ä¿¡æ¯ç›¸ä¼¼åº¦è¶Šé«˜ï¼Œ$softmax$çš„ä½œç”¨æ˜¯ç¡®ä¿æƒé‡ä¹‹å’Œä¸º$1$ï¼Œä¸ºäº†é¿å…è§„æ¨¡å·®å¼‚å¯¹ç›¸ä¼¼åº¦è®¡ç®—äº§ç”Ÿå½±å“ï¼Œé€šå¸¸ä¼šå¯¹å‘é‡è¿›è¡Œç¼©æ”¾ã€‚ä¸ºä»€ä¹ˆè¦é™¤ä»¥$\sqrt{d_k}$ï¼Ÿ

  (2) å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶($Multi$-$Head$ $Self$-$Attention$)ï¼š[CSDN](https://blog.csdn.net/weixin_45662399/article/details/134384186)

  è‡ªæ³¨æ„åŠ›æœºåˆ¶åœ¨å¯¹å½“å‰ä½ç½®ä¿¡æ¯è¿›è¡Œç¼–ç æ—¶ï¼Œä¼šè¿‡åº¦çš„å°†æ³¨æ„åŠ›é›†ä¸­äºè‡ªèº«çš„ä½ç½®ï¼Œå› æ­¤å¼•å…¥å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ã€‚å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„æ­¥éª¤å¦‚ä¸‹ï¼š

  é¦–å…ˆï¼Œå®šä¹‰$n$ç»„$W^Q_nã€W^K_nå’ŒW^V_n$ï¼Œç”Ÿæˆå¤šç»„$Q_nã€K_nå’ŒV_n:$
$$Q_n = X \times W^Q_n$$
$$K_n = X \times W^K_n$$
$$V_n = X \times W^V_n$$
å¯¹å¤šç»„$Qã€Kã€V$è¿›è¡Œ$attention$è®¡ç®—ï¼Œç”Ÿæˆå¤šä¸ª$Attention:$
$$Attention_n(Q_n,K_n,V_n)=softmax(\frac{Q_nK_n^T}{\sqrt{d_k}})V_n$$
å°†å¤šç»„$Attention(Attention_0,Attention_1,...,Attention_n)$è¿›è¡Œæ‹¼æ¥(cancat)ï¼Œå†ä¹˜ä»¥çŸ©é˜µ$W^O$åšä¸€æ¬¡çº¿æ€§å˜æ¢é™ä½ç»´åº¦ï¼Œå¾—åˆ°æœ€ç»ˆçš„$MultiHead:$
$$MultiHead = Concat(Attention_0,Attention_1,...,Attention_n)W^O$$
> [!note] æ³¨é‡Š
> `nhead`éœ€è¦èƒ½è¢«`d_model`æ•´é™¤ï¼Œå› ä¸ºæ‰€æœ‰å¤´çš„ç»´åº¦ä¹‹å’Œç­‰äº`d_model`ã€‚å³æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„è¾“å‡ºç»´åº¦æ˜¯`d_model/nhead`.

  (3) æ®‹å·®ç»“æ„ä¸å‰é¦ˆç¥ç»ç½‘ç»œï¼š

  å‰é¦ˆç¥ç»ç½‘ç»œå…¬å¼ï¼š
$$FFN(x) = max(0,xW_1+b_1)W_2+b_2$$
  > [!note] æ³¨é‡Š
> æ¿€æ´»å‡½æ•°ä¸ºReLUã€‚æ®‹å·®ç»“æ„å‚è€ƒ[CSDN](https://blog.csdn.net/qimo601/article/details/127410607)ã€‚

- **Dncoderéƒ¨åˆ†**

  (1) $Masked$ $Multi$-$Head$ $Self$-$Attention$ï¼š
  
  $mask$-$attention$çš„è®¡ç®—å…¬å¼å¦‚ä¸‹ï¼š
$$Attention_{mask,n}(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}}+mask)V$$
$$
mask = 
\begin{pmatrix}
	0 & -âˆ & -âˆ & \cdots & -âˆ \\
	0 & 0 & -âˆ & \cdots & -âˆ \\
	0 & 0 & 0 & \cdots & -âˆ\\
	\vdots &\vdots& \vdots&\ddots &\vdots\\
	0 & 0 & 0 & \cdots & 0 
\end{pmatrix}
$$
> [!note] æ³¨é‡Š
> ç”±äºtransformeråœ¨è®­ç»ƒæ—¶æ˜¯å°†æ•°æ®ç›´æ¥å…¨éƒ¨è¾“å…¥ç½‘ç»œï¼Œæ‰€ä»¥éœ€è¦ä½¿decoderçœ‹ä¸åˆ°æœªæ¥çš„ä¿¡æ¯ã€‚è®¡ç®—å¾ˆç®€å•ï¼Œå°±æ˜¯åŠ ä¸€ä¸ªmaskçŸ©é˜µ

  (2) $Cross$-$Attention$ï¼š

  $Cross$-$attention$çš„è®¡ç®—å…¬å¼å¦‚ä¸‹ï¼š
$$Attention_n(Q_{Decoder},K_{Encoder},V_{Encoder})=softmax(\frac{QK^T}{\sqrt{d_k}}+mask)V$$
> [!note] æ³¨é‡Š
> Encoderçš„æœ€åä¸€ä¸ªæ¨¡å—çš„è¾“å‡ºï¼Œè¾“å…¥è¿›æ¯ä¸ªDecoderä¸­ï¼Œä½œä¸ºDecoder Multi-Head Self-Attentionçš„$Key$å’Œ$Value$

- è¯¦ç»†è¿‡ç¨‹ï¼š
![[assets/Transformer_Big.png]]

- **Transformerç›¸å…³å˜ä½“**
$LLaMA$

  $LLaMA$æ˜¯ç”±$Meta$å…¬å¸å¼€æºçš„å¤§è¯­è¨€æ¨¡å‹ï¼Œäº$2023$å¹´$2$æœˆ$25$æ—¥å‘å¸ƒï¼ŒåŒ…æ‹¬$7Bã€13Bå’Œ70B$ä¸‰ä¸ªç‰ˆæœ¬ã€‚$LLaMA$æ˜¯Transformerçš„å˜ä½“ï¼Œç›¸æ¯”Transformerï¼Œå…¶ç»“æ„ä¸ç‰¹ç‚¹å¦‚ä¸‹ï¼š

  (1) $Encoder$-$Decoder$ $\Rightarrow$ $Decoder$-$Only$ï¼š
  > [!note] æ³¨é‡Š
> Llamaæ¨¡å‹ä»…é‡‡ç”¨äº†Transfomerçš„è§£ç å™¨ç»“æ„ï¼Œç›¸æ¯”Transfomerçš„æ ·æœ¬å’Œæ ‡ç­¾æ•°æ®åˆ†åˆ«ä»ç¼–ç å™¨å’Œè§£ç å™¨è¾“å…¥ï¼Œ$LLaMA$ç›´æ¥å°†æ ·æœ¬å’Œæ ‡ç­¾æ‹¼æ¥åœ¨ä¸€èµ·è¾“å…¥åˆ°ç¥ç»ç½‘ç»œ

  (2) $LayerNorm$ $and$ $Post$-$norm$ $\Rightarrow$ $RMSNorm$ $and$ $Pre$-$norm$ï¼š
  > [!note] æ³¨é‡Š
> ç›¸æ¯”Transformçš„$LayerNorm$ï¼Œ$LLaMA$æ¨¡å‹é‡‡ç”¨$RMSNorm$ï¼Œ$RMSNorm$æ¯”$Layernorm$è®¡ç®—æ›´ç®€ä¾¿ï¼ŒèŠ‚çº¦äº†è®¡ç®—é€Ÿåº¦ï¼Œæ­¤å¤–ï¼Œ$LLaMA$è¿˜å°†$nomilization$å±‚æ”¾åœ¨äº†æ³¨æ„åŠ›çš„å‰é¢ï¼Œä¸ºå•¥ä¸çŸ¥é“

  (3) ä¸‰è§’å‡½æ•°ç¼–ç  â†’ æ—‹è½¬ä½ç½®ç¼–ç (RoPE)ï¼š[è®ºæ–‡](http://arxiv.org/abs/2104.09864)[ç§‘å­¦ç©ºé—´](https://kexue.fm/archives/10040)

   ä»¥äºŒç»´è¯å‘é‡ä¸ºä¾‹ï¼Œå°†è‡ªæ³¨æ„åŠ›è®¡ç®—è¿‡ç¨‹ä¸­çš„çš„$q$å’Œ$k$ä¸äºŒç»´æ—‹è½¬çŸ©é˜µ$R^{2Ã—2}$ç›¸ä¹˜ï¼š
$$
R_n^{2Ã—2} = 
\begin{pmatrix}
	cos(nÎ¸) & -sin(nÎ¸) \\
	sin(nÎ¸) & cos(nÎ¸)
\end{pmatrix}
$$
$$(R_m^{2Ã—2}q)^âŠ¤(R_n^{2Ã—2}k)=q^{âŠ¤}(R^{2Ã—2}_m)^TR_n^{2Ã—2}k=q^âŠ¤R_{nâˆ’m}^{2Ã—2}k$$
> [!note] æ³¨é‡Š
> å…¶ä¸­ï¼Œ$Î¸$æ˜¯ä¸€ä¸ªå¸¸é‡ï¼Œå®ƒçš„å…·ä½“åŸç†æ˜¯è®©è¯å‘é‡åœ¨ç©ºé—´ä¸Šå‘ç”Ÿæ—‹è½¬ï¼Œä»è€ŒæŠŠä½ç½®ä¿¡æ¯åµŒå…¥åˆ°è¯å‘é‡ä¸­ï¼Œè¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆå®ƒå«æ—‹è½¬ä½ç½®ç¼–ç ã€‚å¯ä»¥çœ‹åˆ°ç›¸å¯¹ä½ç½®ä¿¡æ¯$(nâˆ’m)$åœ¨$q$ä¸$k$ç›¸ä¹˜ååµŒå…¥åˆ°äº†è¯å‘é‡ä¸­ï¼Œå½¢è±¡è¿‡ç¨‹å¯ä»¥çœ‹ä¸‹å›¾ï¼š

![[assets/RoPE.png|500]]

- è¯å‘é‡ä¸€èˆ¬ä¸å¯èƒ½æ˜¯äºŒç»´ï¼Œè¦æƒ³å¯¹é«˜ç»´è¯å‘é‡è¿›è¡Œæ—‹è½¬ï¼Œéœ€è¦å°†æ—‹è½¬çŸ©é˜µæ‹“å±•åˆ°é«˜ç»´ï¼š

$$R_n^{dÃ—d} =
\begin{pmatrix}
	cos(nÎ¸_0) & -sin(nÎ¸_0) & 0 & 0 & \cdots & 0 & 0\\
	sin(nÎ¸_0) & cos(nÎ¸_0) & 0 & 0 & \cdots & 0 & 0\\
	0 & 0 & sin(nÎ¸_1) & cos(nÎ¸_1) & \cdots & 0 & 0\\
	0 & 0 & sin(nÎ¸_1) & cos(nÎ¸_1) & \cdots & 0 & 0\\
	\vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\
	0 & 0 & 0 & 0 & \cdots & cos(nÎ¸_{d/2-1}) & -sin(nÎ¸_{d/2-1})\\
	0 & 0 & 0 & 0 & \cdots & sin(nÎ¸_{d/2-1}) & cos(nÎ¸_{d/2-1})\\
\end{pmatrix}
$$

- å¯ä»¥åº”ç”¨åˆ†å—çŸ©é˜µå°†æ—‹è½¬çŸ©é˜µç®€åŒ–ï¼š

$$
\begin{align}
R_n^{dÃ—d}
&=
\left( 
\begin{array}{cc|ccc}
	cos(nÎ¸_0) & -sin(nÎ¸_0) & 0 & 0 & \cdots & 0 & 0\\
	sin(nÎ¸_0) & cos(nÎ¸_0) & 0 & 0 & \cdots & 0 & 0\\
	\hline 
	0 & 0 & sin(nÎ¸_1) & cos(nÎ¸_1) & \cdots & 0 & 0\\
	0 & 0 & sin(nÎ¸_1) & cos(nÎ¸_1) & \cdots & 0 & 0\\
	\vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\
	0 & 0 & 0 & 0 & \cdots & cos(nÎ¸_{d/2-1}) & -sin(nÎ¸_{d/2-1})\\
	0 & 0 & 0 & 0 & \cdots & sin(nÎ¸_{d/2-1}) & cos(nÎ¸_{d/2-1})
\end{array}
\right)\\\\
&= 
\begin{pmatrix}
	R_{(0)} & 0 & 0 & \cdots & 0\\
	0 & R_{(1)} & 0 & \cdots & 0\\
	0 & 0 & R_{(2)} & \cdots & 0\\
	0 & 0 & 0 & \ddots & \vdots\\
	0 & 0 & 0 & \cdots & R_{(d/2-1)}
\end{pmatrix}
\end{align}
$$
- è®¡ç®—å…¬å¼æ”¹å†™ä¸ºä»¥ä¸‹å½¢å¼ï¼Œä¸å±•å¼€äº†ï¼š
$$(R_m^{dÃ—d}q)^âŠ¤(R_n^{dÃ—d}k)=q^{âŠ¤}(R^{dÃ—d}_m)^TR_n^{dÃ—d}k=q^âŠ¤R_{nâˆ’m}^{dÃ—d}k$$
> [!note] æ³¨é‡Š
> promptçš„ç†è§£æ˜¯å¦å¤–ä¸€ä¸ªå°å‹çš„è®­ç»ƒæ ·æœ¬é€‚æ—¶çš„è°ƒæ•´å¤§æ¨¡å‹çš„å‚æ•°

- $BERT$

  (1) $Encoder$-$Decoder$ $\Rightarrow$ $Encoder$-$Only$ï¼š
  > [!note] æ³¨é‡Š
> BERTæ¨¡å‹å°±æ˜¯Transformerå·¦è¾¹çš„Encodeerç»“æ„ï¼Œ2018å¹´10â½‰ç”±Google AIç ”ç©¶é™¢æå‡ºï¼Œå…¶æ¶æ„å›¾å¦‚ä¸‹ï¼š

![[assets/Bert.png|600]]

- $GPT$

  (1) $Encoder$-$Decoder$ $\Rightarrow$ $Decoder$-$Only$ï¼š
  > [!note] æ³¨é‡Š
> GPTæ¨¡å‹ï¼Œå…¨ç§°ä¸ºâ€Œ(Generative Pre-trained Transformer)ï¼Œæ˜¯ç”±â€ŒOpenAIå›¢é˜Ÿå¼€å‘çš„ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„è‡ªç„¶è¯­è¨€å¤„ç†æ¨¡å‹ã€‚GPTä½¿ç”¨Transformerçš„Decoderä½œä¸ºå…¶æ¶æ„ï¼Œä½†åœ¨Decoderçš„åŸºç¡€ä¸Šåšäº†ä¸€äº›æ”¹åŠ¨ï¼Œå»æ‰äº†Cross-Attentionã€‚

- T5æ¨¡å‹(Transfer Text-to-Text Transformer)
è®­ç»ƒæ•°æ®ï¼šé«˜è´¨é‡æ•°æ®C4

---

![[assets/v2-7e06234980fe84aca696efceab3a5b06_1440w.png|800]]
å…¨å‚å¾®è°ƒã€LoRAå¾®è°ƒ